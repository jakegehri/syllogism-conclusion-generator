{
   "cell_type": "code",
   "execution_count": 1,
   "id": "778e9f07-e582-4797-9da5-6c4259a6216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba9b2a-ca8a-4652-933c-dfb8573558b9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13042d17-38a8-4d13-92f1-21cb2bbe189a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook will be an extension of the last notebook that was working to classify whether two premises could be used to generate a valid conclusion. That model used the a BERT architecture and was fine-tuned on the Avicenna syllogism dataset. This notebook will use the same dataset, but instead fine-tune a GPT-2 model to take in two premises as input and generate the corresponding conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3dd3d9-b4e5-4168-91d3-c48f8bde7039",
   "metadata": {},
   "source": [
    "I had to write custome start, end and pad tokens in order to properly pad each input as to be forced to randomly chop up syllogisms into pieced and create input blocks of equal size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bafaebb-e04f-4629-90c4-a8cec72a9698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255d8effb8d945c6965a214a72620efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b689146ce2e43259e7f8816ebfb6af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4cab5f774a457d8608823a23001366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b103ab121e4abbb48c049bbd66a6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "file_name = 'Avicenna_Train.csv'\n",
    "model_cp = \"gpt2\"\n",
    "max_length = 200\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_cp, bos_token = '<startoftext>', \n",
    "                                          eos_token='<endoftext>', pad_token='<pad>')\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors='pt')\n",
    "model = GPT2LMHeadModel.from_pretrained(model_cp).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bebd84-e61e-443c-a887-9bd4ad650efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, max_length=max_length, padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122deb1-a091-4259-ba95-9ad7fbc23b63",
   "metadata": {},
   "source": [
    "The data came in a csv file containing premise 1, premise 2, validity and conclusion. I needed to filter the dataset removing all invalid syllogisms and then combine the premises and conclusions into a single string for fine-tuning. I found out that telling the model which premise was which and where the conclusion started improved training. Additionally, adding a $ after the last premise slightly imporved training, but this was moreso done to replicate what as done in the original GPT paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56570068-af79-44b3-89bb-c5a1827ea11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(file_name):\n",
    "    dataset = load_dataset('csv', data_files=file_name, sep = ',', encoding = 'ISO-8859-1')\n",
    "    dataset.set_format(type='pandas')\n",
    "    df = dataset['train'][:]\n",
    "    df = df[df['Syllogistic relation'] == 'yes']\n",
    "    df['text'] = '<startoftext>' + 'Premise 1: ' + df['Premise 1'] + 'Premise 2:' + df['Premise 2'] + '$' + 'Conclusion:' + df['Conclusion'] + '<endoftext>'\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df[['text']]\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36b3faca-a580-461e-8955-ae2b02dc506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9e35c2288d530357\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-9e35c2288d530357/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b5bb41dd5b49f0988b2204bd1a751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1206f18026244e6093596c5213272a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140fbce558d843f4a09c6f24019c9762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db6c5fc150d46dd93240e153414e576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbf3b0566364421809a41a495e6b2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-80959f65edc13f7a\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80959f65edc13f7a/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec743c282bb84dc0872e6461014edbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2cc281914d454fa5b9f4103c4a0460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caecf74dec534c278bdc66c4ccd7f02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a2302ba5944aa6872be398266c2c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc13db456ed6430abe86da7dd206c87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = prepare_dataset('Avicenna_Train.csv')\n",
    "test_dataset = prepare_dataset('Avicenna_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2209965d-d6c9-46f8-ab81-31d15b6e8f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<startoftext> Premise 1: Chronic diseases are heart attacks and stroke, cancer such as breast and colon cancer, diabetes, epilepsy and seizures, obesity, and oral health problems.Premise 2:In populations that eat a regular high-fiber diet of more than 50 grams of fiber per dayTrusted Source, like rural South Africans, chronic diseases are very low.$Conclusion:In populations that eat a regular high-fiber diet of more than 50 grams of fiber per dayTrusted Source, like rural South Africans, heart attacks and stroke, cancer such as breast and colon cancer, diabetes, epilepsy and seizures, obesity, and oral health problems are very low. <endoftext> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d665888-31f2-4935-a49f-41f58e53bceb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66ebfa2e-6569-4ee5-bcd9-78d5e0225e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_cp.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_cp}-finetuned-syllogism\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "500b719c-13fe-4f11-93a5-fff1ebf01f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b8af073-0f00-445c-9a1a-1b494c9deb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2427\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='912' max='912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [912/912 04:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.311477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.267000</td>\n",
       "      <td>2.312688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.267000</td>\n",
       "      <td>2.314481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to gpt2-finetuned-syllogism/checkpoint-500\n",
      "Configuration saved in gpt2-finetuned-syllogism/checkpoint-500/config.json\n",
      "Model weights saved in gpt2-finetuned-syllogism/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=912, training_loss=2.215385637785259, metrics={'train_runtime': 258.8719, 'train_samples_per_second': 28.126, 'train_steps_per_second': 3.523, 'total_flos': 743151283200000.0, 'train_loss': 2.215385637785259, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "467bd632-d570-472f-9819-de653d6a25cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 10.12\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9909f-641a-4763-9de8-6648e6c5842e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7b24d-5b1c-4d92-8ba7-0ded7fe51e71",
   "metadata": {},
   "source": [
    "First a classic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18a22afd-3c09-4ab4-9336-f330978dead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a6b00ec1-d523-45ce-8c9f-454f5ca14781",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(test, return_tensors='pt')['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6177045f-2eab-42b7-a0d6-a07ebf13e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy = model.generate(input_ids, max_length=25)\n",
    "tokenizer.decode(output_greedy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8747997-63c5-488d-b625-98a039976d7e",
   "metadata": {},
   "source": [
    "I dont know why it is generating this error, I believe it has something to do with me changing the models innate tokens in the beginning. But we can see that the model was able to accurately generate the conclusion for this syllogism. This first example is using greedy search, where our model simply makes a next word prediction based on the our probability distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc705cf3-801b-47b2-a9ec-775b9d1d5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=25, num_beams=5)\n",
    "tokenizer.decode(output_beam[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea53203f-7c5f-425f-bc3c-63c7aa4d250d",
   "metadata": {},
   "source": [
    "Beam search is when the model generates n number of 'beams' or full sentence predictions (in this case 5) and then a word is decided based on highest probability and we continue moving down the rest of the sentence, not going back to earlier ones. This model also looks good. Beam will usually outperform greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f126a2-3716-413e-9fe5-3d472a0955df",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b79d2b-4e46-447f-b9cb-09a6f9a48382",
   "metadata": {},
   "source": [
    "See below for more tests and search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d76c1a53-b82e-4c52-bea4-27f8c19d6016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=25, do_sample=True, temperature = 0.5)\n",
    "tokenizer.decode(output_temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2fc326f2-b431-44b2-b734-038e9c87a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is a'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, max_length=25, do_sample=True, top_k=50)\n",
    "tokenizer.decode(output_topk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c303fc26-6579-44c8-97d3-6d983852a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=25, do_sample=True, top_p=0.90)\n",
    "tokenizer.decode(output_topp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3f4951d-12cf-4929-b7cb-fa66688d9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = 'Premise 1: All mammals are animals. Premise 2: All elephants are mammals. $ Conclusion: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d3db2d4-5dca-49d4-9a5f-ea62b830b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(test2, return_tensors='pt')['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d37a804b-5da3-4de2-83fa-f0d8c093d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All mammals are animals. Premise 2: All elephants are mammals. $ Conclusion:  All elephants are animals. <endoftext>                       '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy = model.generate(input_ids, max_length = 50)\n",
    "tokenizer.decode(output_greedy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b79e672-b409-4e80-b05e-b3f31afe332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All mammals are animals. Premise 2: All elephants are mammals. $ Conclusion:  All elephants are animals. <endoftext>                       '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length = 50, num_beams=5)\n",
    "tokenizer.decode(output_beam[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83fc48ff-1c3e-4a3a-8e4a-b3a77039f052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Premise 1: All mammals are warm-blooded. Premise 2: All black dogs are mammals. $ Conclusion:  All black dogs are warm-blooded. <endoftext>   <endoftext>  animal is warm-'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = 'Premise 1: All mammals are warm-blooded. Premise 2: All black dogs are mammals. $ Conclusion: '\n",
    "input_ids = tokenizer(test3, return_tensors='pt')['input_ids'].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=40, num_beams = 5)\n",
    "tokenizer.decode(output_beam[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
